{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "\n",
    "Code from Assignment 3 for tweet sentiment analysis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\satya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\satya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    desertfoxi thi rich nnfirst unleash wuhancoron...\n",
       "1    jack a number satir poster featur xi jin ping ...\n",
       "2    bin meantim peopl die lack access medic treatm...\n",
       "3    desertfoxi thi rich nnfirst unleash wuhancoron...\n",
       "4    bmpiaind step back afteral happen relat wuhanc...\n",
       "Name: tweet_data, dtype: object"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentiment Analysis Data\n",
    "\n",
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "#using twitter Authentication to connect to twitter api with twitter provided keys.\n",
    "authentication = tweepy.OAuthHandler('Dpp7E4W9YkKGVszqezpjpSlLc', 'bczWmE17Ve1DJHKJY0Z9eQYuAupPZ63JfL9AGnkuX12SUZkCWJ')\n",
    "authentication.set_access_token('4289066089-Vd6LpZcoLUERdoFJTnk3J12w8M2sgajQ8HDIlaL', 'yebOZX98i8RtrsMNcWrLBh54K6ypZXi3KejgXSzunyxnH')\n",
    "api = tweepy.API(authentication,wait_on_rate_limit=True)\n",
    "\n",
    "#file operation\n",
    "csvFile = open('tagdata.csv', 'w')\n",
    "data = pd.read_csv('tagdata.csv', names=['tweet_data'], header=None)\n",
    "write_to_csv = csv.writer(csvFile)\n",
    "\n",
    "#writing to file using csvwriter\n",
    "for tweet_data in tweepy.Cursor(api.search,q=\"#wuhancoronavirus\",lang=\"en\").items(100):\n",
    "  write_to_csv.writerow([tweet_data.text.encode('utf-8')])\n",
    "#reading csv file \n",
    "data = pd.read_csv('tagdata.csv',error_bad_lines=False,names=['tweet_data'])\n",
    "import string\n",
    "\n",
    "#Removing noise (Punctuation,extra words)\n",
    "data['tweet_data']=data['tweet_data'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "data['tweet_data']=data['tweet_data'].str.replace('[^A-Za-z ]','')\n",
    "data['tweet_data']=data['tweet_data'].str.replace('bRT','')\n",
    "# print(data['tweet_data'].head())\n",
    "\n",
    "#Removing numbers\n",
    "data['tweet_data'] = data['tweet_data'].str.replace('\\d+', '')\n",
    "\n",
    "#Removing StopWords\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop=stopwords.words('english')\n",
    "data['tweet_data']=data['tweet_data'].apply(lambda x:\" \".join(y for y in x.split() if y not in stop))\n",
    "\n",
    "\n",
    "#Lower casing\n",
    "data['tweet_data']=data['tweet_data'].str.lower()\n",
    "data2=data[\"tweet_data\"]\n",
    "#stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "st=PorterStemmer()\n",
    "data['tweet_data']=data['tweet_data'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
    "\n",
    "#Lemmetization\n",
    "from textblob import Word\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "data['tweet_data']=data['tweet_data'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "data['tweet_data'].head(5)\n",
    "#saving the formatted sentences to \"Formatted_sentences.txt\"\n",
    "# data['tweet_data'].to_csv(\"Formatted_tweets.csv\",header=\"Tweets\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from textblob import TextBlob \n",
    "\n",
    "def clean_tweet(tweet): \n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) \n",
    "  \n",
    "def get_tweet_sentiment(tweet): \n",
    "        ''' \n",
    "        Utility function to classify sentiment of passed tweet \n",
    "        using textblob's sentiment method \n",
    "        '''\n",
    "        # create TextBlob object of passed tweet text \n",
    "        analysis = TextBlob(clean_tweet(tweet)) \n",
    "        # set sentiment \n",
    "        if analysis.sentiment.polarity > 0: \n",
    "            return 'positive'\n",
    "        elif analysis.sentiment.polarity == 0: \n",
    "            return 'neutral'\n",
    "        else: \n",
    "            return 'negative'\n",
    "filePointer = open(\"Sentimetn_Analysis_File.csv\",\"w\")\n",
    "                               \n",
    "sentiment_data={}  \n",
    "counter=1\n",
    "for tweet in data[\"tweet_data\"]:\n",
    "#     sentiment_data[tweet]=get_tweet_sentiment(tweet)\n",
    "    data=str(counter)+\",\"+str(tweet)+\",\"+str(get_tweet_sentiment(tweet))+\"\\n\"\n",
    "    filePointer.write(data)\n",
    "    counter=counter+1\n",
    "filePointer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question 1: Topic Modeling\n",
    "\n",
    "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to identify the top 10 topics in the dataset. Before answering this question, please review the materials in lesson 8, especially the code for LDA and LSA. The following information should be reported:\n",
    "\n",
    "### (1) Features (top n-gram phrases) used for topic modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bthi', 'rich', 'nnfirst', 'unleash', 'wuhancoronaviru', 'world', 'nnthen', 'suppli', 'substandard', 'medic', 'equip', 'itemxexxa']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return ps.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result\n",
    "processed_docs=list()\n",
    "for i in trainData['tweet']:\n",
    "    processed_docs.append(preprocess(i))\n",
    "\n",
    "bigram = gensim.models.Phrases(processed_docs, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[processed_docs], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "print(trigram_mod[bigram_mod[processed_docs[0]]])\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Top 10 clusters for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.060*\"wuhancoronaviru\" + 0.059*\"wuhan\" + 0.059*\"million\" + 0.053*\"hunterwolfwang\" + 0.053*\"hide\" + 0.053*\"inform\" + 0.053*\"form\" + 0.040*\"peopl\" + 0.040*\"wholxexxa\" + 0.020*\"starvancouv\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.033*\"case\" + 0.033*\"million\" + 0.033*\"markn\" + 0.033*\"cross\" + 0.033*\"week\" + 0.033*\"effort\" + 0.033*\"corona\" + 0.031*\"infect\" + 0.022*\"txexxa\" + 0.022*\"bblu\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.053*\"wuhancoronaviru\" + 0.018*\"covid\" + 0.018*\"spread\" + 0.018*\"china\" + 0.010*\"sxexxa\" + 0.010*\"great\" + 0.010*\"breptedyoho\" + 0.010*\"perform\" + 0.010*\"houseforeigngop\" + 0.010*\"contain\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.030*\"wuhancoronaviru\" + 0.020*\"thing\" + 0.020*\"peopl\" + 0.020*\"typic\" + 0.011*\"canadian\" + 0.011*\"thank\" + 0.011*\"think\" + 0.011*\"video\" + 0.011*\"mask\" + 0.011*\"social\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.076*\"wuhancoronaviru\" + 0.056*\"infect\" + 0.056*\"shabnamhamseda\" + 0.056*\"close\" + 0.056*\"ccpchina\" + 0.056*\"alli\" + 0.056*\"trudeau\" + 0.056*\"terrorist\" + 0.056*\"entirexexxa\" + 0.056*\"reengin\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.046*\"wuhancoronaviru\" + 0.027*\"china\" + 0.027*\"human\" + 0.020*\"becau\" + 0.020*\"wuhan\" + 0.015*\"infect\" + 0.014*\"transmiss\" + 0.014*\"nnbecau\" + 0.014*\"desertfoxi\" + 0.014*\"overxexxa\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.033*\"hope\" + 0.033*\"wherewith\" + 0.033*\"doubt\" + 0.033*\"power\" + 0.033*\"turn\" + 0.033*\"suspicion\" + 0.033*\"purview\" + 0.033*\"dsgtwilson\" + 0.033*\"wthe\" + 0.033*\"everxexxa\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.028*\"home\" + 0.028*\"realjameswood\" + 0.028*\"suddenli\" + 0.028*\"youxexxr\" + 0.028*\"bexexxa\" + 0.028*\"gander\" + 0.028*\"globalist\" + 0.028*\"wonder\" + 0.028*\"cartel\" + 0.028*\"unemploy\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.024*\"wuhancoronaviru\" + 0.024*\"wuhanviru\" + 0.024*\"nafter\" + 0.024*\"foxbusi\" + 0.024*\"nnfor\" + 0.024*\"matter\" + 0.024*\"medium\" + 0.024*\"suspend\" + 0.024*\"drmartyfox\" + 0.024*\"trishregan\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.064*\"wuhancoronaviru\" + 0.059*\"realjameswood\" + 0.059*\"usual\" + 0.059*\"reveal\" + 0.059*\"drug\" + 0.059*\"tsunami\" + 0.059*\"anoth\" + 0.059*\"lock\" + 0.059*\"southern\" + 0.059*\"border\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics =10 , \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Summarize and describe the topic for each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 - Virus Effect, describing about the effect of corona on people\n",
    "\n",
    "1 - Virus Cases, describing the cases per week\n",
    "\n",
    "2 - China over Virus, describing about how china handled the virus\n",
    "\n",
    "3 - Political - describing political activities\n",
    "\n",
    "4 - Terrorism - describing terror activities\n",
    "\n",
    "5 - Infections - about the infections\n",
    "\n",
    "6 - Political\n",
    "\n",
    "7 - Unemployment - about the unemployment issue due to corona\n",
    "\n",
    "8 - virus Impact - Imapct of virus stages\n",
    "\n",
    "9 - Corona drug - about the virus drug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Sentiment Analysis\n",
    "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: 80% data for training and 20% data for testing.\n",
    "\n",
    "(1) Features used for sentiment classification and explain why you select these features.\n",
    "\n",
    "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively.\n",
    "\n",
    "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# train Data\n",
    "Data = pd.read_csv(\"Sentimetn_Analysis_File.csv\",names=[\"id\",\"tweet\",\"label\"],header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create feature vectors\n",
    "y=Data.label\n",
    "trainData=Data\n",
    "x=Data.drop('label',axis=1)\n",
    "tesData=Data\n",
    "vectorizer = TfidfVectorizer(min_df = 5,\n",
    "                             max_df = 0.8,\n",
    "                             sublinear_tf = True,\n",
    "                             use_idf = True)\n",
    "train_vectors = vectorizer.fit_transform(trainData['tweet'])\n",
    "test_vectors = vectorizer.transform(testData['tweet'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_vectors, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively.\n",
    "### (3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 0.001994s; Prediction time: 0.000998s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00         2\n",
      "     neutral       0.75      1.00      0.86         6\n",
      "    positive       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.82        11\n",
      "   macro avg       0.58      0.67      0.62        11\n",
      "weighted avg       0.68      0.82      0.74        11\n",
      "\n",
      "0.8181818181818182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\satya\\Anaconda3\\envs\\tensorflow-sessions\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Linear Algorithm\n",
    "\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(X_test)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "# results\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "\n",
    "print(classification_report(y_test, prediction_linear))\n",
    "print(accuracy_score(y_test, prediction_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.50      0.67         2\n",
      "     neutral       0.86      1.00      0.92         6\n",
      "    positive       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.91        11\n",
      "   macro avg       0.95      0.83      0.86        11\n",
      "weighted avg       0.92      0.91      0.90        11\n",
      "\n",
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Algorithm\n",
    "\n",
    "y=Data.label\n",
    "x=Data.drop('label',axis=1)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\n",
    "train_vectors = vectorizer.fit_transform(trainData['tweet'])\n",
    "test_vectors = vectorizer.transform(testData['tweet'])\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_vectors, y, test_size=0.2, random_state=0)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "text_classifier = RandomForestClassifier(n_estimators=200, random_state=0)\n",
    "text_classifier.fit(X_train, y_train)\n",
    "predictions = text_classifier.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(classification_report(y_test,predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing both results, both algorithms performed almost same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
