{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/psameni/Pranaya_INFO5731_Spring2020/blob/master/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O70jm8LXJwm",
        "colab_type": "text"
      },
      "source": [
        "INFO5731 Assignment Two\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data.\n",
        "\n",
        "Question 1\n",
        "(40 points). Write a python program to collect text data from either of the following sources and save the data into a csv file:\n",
        "\n",
        "(1) Collect all the customer reviews of the product 2019 Dell labtop on amazon.\n",
        "\n",
        "(2) Collect the top 100 User Reviews of the film Joker from IMDB.\n",
        "\n",
        "(3) Collect the abstracts of the top 100 research papers by using the query natural language processing from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 100 tweets by using hashtag \"#wuhancoronovirus\" from Twitter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOeHRLLGWClh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tweepy\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "#using twitter Authentication to connect to twitter api with twitter provided keys.\n",
        "authentication = tweepy.OAuthHandler('Dpp7E4W9YkKGVszqezpjpSlLc', 'bczWmE17Ve1DJHKJY0Z9eQYuAupPZ63JfL9AGnkuX12SUZkCWJ')\n",
        "authentication.set_access_token('4289066089-Vd6LpZcoLUERdoFJTnk3J12w8M2sgajQ8HDIlaL', 'yebOZX98i8RtrsMNcWrLBh54K6ypZXi3KejgXSzunyxnH')\n",
        "api = tweepy.API(authentication,wait_on_rate_limit=True)\n",
        "\n",
        "#file operation\n",
        "csvFile = open('tagdata.csv', 'w')\n",
        "data = pd.read_csv('tagdata.csv', names=['tweet_data'], header=None)\n",
        "write_to_csv = csv.writer(csvFile)\n",
        "\n",
        "#writing to file using csvwriter\n",
        "for tweet_data in tweepy.Cursor(api.search,q=\"#wuhancoronavirus\",lang=\"en\").items(100):\n",
        "  write_to_csv.writerow([tweet_data.text.encode('utf-8')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaHwt2O2XQRH",
        "colab_type": "text"
      },
      "source": [
        "Question 2\n",
        "(30 points). Write a python program to clean the text data you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IV25iV8AWCls",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "4f4f9474-45ac-4742-a8f5-a9876cf1629c"
      },
      "source": [
        "#reading csv file \n",
        "data = pd.read_csv('tagdata.csv',error_bad_lines=False,names=['tweet_data'])\n",
        "\n",
        "import string\n",
        "\n",
        "#Removing noise (Punctuation,extra words)\n",
        "data['tweet_data']=data['tweet_data'].str.replace('[{}]'.format(string.punctuation), '')\n",
        "data['tweet_data']=data['tweet_data'].str.replace('[^A-Za-z ]','')\n",
        "data['tweet_data']=data['tweet_data'].str.replace('bRT','')\n",
        "# print(data['tweet_data'].head())\n",
        "\n",
        "#Removing numbers\n",
        "data['tweet_data'] = data['tweet_data'].str.replace('\\d+', '')\n",
        "\n",
        "#Removing StopWords\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop=stopwords.words('english')\n",
        "data['tweet_data']=data['tweet_data'].apply(lambda x:\" \".join(x for x in str(x).split() if x not in stop))\n",
        "\n",
        "\n",
        "#Lower casing\n",
        "data['tweet_data']=data['tweet_data'].str.lower()\n",
        "\n",
        "#stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "st=PorterStemmer()\n",
        "data['tweet_data']=data['tweet_data'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "\n",
        "#Lemmetization\n",
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "data['tweet_data']=data['tweet_data'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "print(data['tweet_data'].head(5))\n",
        "#saving the formatted sentences to \"Formatted_sentences.txt\"\n",
        "data['tweet_data'].to_csv(\"Formatted_tweets.csv\",header=\"Tweets\",index=False)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "0    enzomazak will end humanitynccpchina destroy p...\n",
            "1    officialmin chine dog slaughterhous part nnhtt...\n",
            "2    wbyeat china no vaccin cure medic suppli isol ...\n",
            "3    enzomazak will end humanitynccpchina destroy p...\n",
            "4    bnhknew china communist hide truth spread viru...\n",
            "Name: tweet_data, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLgnsSy1XYBk",
        "colab_type": "text"
      },
      "source": [
        "Question 3\n",
        "(30 points). Write a python program to conduct syntax and structure analysis of the clean text you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtmPiuniWCl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "2d2f26be-7ab4-4a57-e073-9f110d3ba100"
      },
      "source": [
        "import csv\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize \n",
        "nltk.download('punkt')\n",
        "\n",
        "tweetData = str(data['tweet_data'])\n",
        "\n",
        "#tokenize the sentences\n",
        "tokens = nltk.word_tokenize(tweetData)\n",
        "\n",
        "#Parts of Speech\n",
        "tags_data = nltk.pos_tag(tokens)\n",
        "\n",
        "print(\"POS tags :\",tags_data[:10])\n",
        "\n",
        "# no of nouns, noun plurals\n",
        "noun = [item for item in tags_data if (item[1] =='NN' or item[1]=='NNS')] \n",
        "noun_count = len(noun)\n",
        "print(\"No of Nouns :\",noun_count)\n",
        "\n",
        "# no of adjectives\n",
        "JJ = [item for item in tags_data if item[1] =='JJ'] \n",
        "adjectives_count = len(JJ)\n",
        "print(\"No of Adjectives :\",adjectives_count)\n",
        "\n",
        "\n",
        "# no of verbs, past participle taken\n",
        "VBP = [item for item in tags_data if (item[1] =='VBP' or item[1] =='VBN')]\n",
        "verb_count = len(VBP)\n",
        "print(\"No of Verbs :\",verb_count)\n",
        "\n",
        "#no of dates\n",
        "dates = [item for item in tags_data if item[1] =='JJ' or item[1] =='CD'] \n",
        "date_count = len(dates)\n",
        "print(\"No of dates :\",date_count)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "POS tags : [('0', 'CD'), ('enzomazak', 'NN'), ('will', 'MD'), ('end', 'VB'), ('humanitynccpchina', 'NN'), ('destroy', 'NN'), ('p', 'NN'), ('...', ':'), ('1', 'CD'), ('officialmin', 'JJ')]\n",
            "No of Nouns : 46\n",
            "No of Adjectives : 12\n",
            "No of Verbs : 0\n",
            "No of dates : 23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHTftC4kWCl-",
        "colab_type": "text"
      },
      "source": [
        "### Constituency Parsing and Dependency Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2jFIpVWWCmA",
        "colab_type": "text"
      },
      "source": [
        "Constituency Parsing captures the relationship between words and phrases and then eventually between phrases. In the example \"The smart kid follows the trend\", S(Sentence) is separated into NP(Noun Phrase) and VP(Verb Phrase). The rest of it works as same as this way.\n",
        "\n",
        "Dependency Parsing captures only the dependency between the words.The dependency relation has a structure of a triple. It has head, relation and dependent. In other words, a dependent depends on a head according to a certain relation. In the example \"The smart kid follows the trend\", \n",
        "In Dependency Parsing Result - in head:?, relation:?, dependent:? format\n",
        "head: chased, relation: nsubj, dependent: cat\n",
        "head: cat, relation: det, dependent: The\n",
        "The rest of it works as same as this way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMLIQXEsWCmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}